{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12-05-2018-cm-train-lstm-model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vgzE8agC0fnN",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Model hyperparameters\n",
        "num_train = 600 #@param {type:\"slider\", min:2, max:600, step:2}\n",
        "learning_rate = 0.008 #@param {type:\"slider\", min:0.005, max:0.1, step:0.001}\n",
        "num_epochs = 30 #@param {type:\"slider\", min:10, max:300, step:10}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uSN6JBRrKUGX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install and Load Package Dependencies"
      ]
    },
    {
      "metadata": {
        "id": "1FR7OlKvgATx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install pytorch\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "# Install gensim, more recent seaborn, slack API wrapper\n",
        "!pip install gensim seaborn==0.9.0 slacker"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5sw330E0rMs8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You will need your own Slack token to utilize the Slack API. "
      ]
    },
    {
      "metadata": {
        "id": "8yxjwx5SfOxa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from string import whitespace\n",
        "import time\n",
        "\n",
        "# arrays and dataframes\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import seaborn as sns\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "# pytorch: neural nets including LSTMs\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# gensim: word embeddings w/ word2vec\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "# google drive api\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "from slacker import Slacker\n",
        "slack = Slacker('YOUR_TOKEN_HERE')\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9k9eInx7KJlB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import data and class labels"
      ]
    },
    {
      "metadata": {
        "id": "i_WNNm0bUmuV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define data filepaths and read\n",
        "drive_dir = '/gdrive/My Drive/med277/'\n",
        "\n",
        "tags_filename = '20181204_MR_ncbi-disease-corpus_bio-translation.txt'\n",
        "tags_filepath = tags_filename\n",
        "corpus_tag = open(tags_filepath, 'r')\n",
        "\n",
        "text_filename = '20181204_MR_ncbi-disease-corpus_text.txt'\n",
        "text_filepath = text_filename\n",
        "corpus_text = open(text_filepath, 'r')\n",
        "\n",
        "# format input text\n",
        "corpus_list = []\n",
        "for entry in corpus_text.readlines():\n",
        "    a = entry.split('\\t')[1:]\n",
        "    a = [word.strip() for word in a]\n",
        "    corpus_list.append(a)\n",
        "    \n",
        "# format input tags\n",
        "tag_list = []\n",
        "for entry in corpus_tag.readlines():\n",
        "    a = entry.split('\\t')[1:]\n",
        "    a = [tag.strip() for tag in a]\n",
        "    tag_list.append(a)\n",
        "    \n",
        "# remove entries that don't parse correctly\n",
        "print('Corpus size: {}'.format(len(corpus_list)))\n",
        "remove_a = []\n",
        "remove_b = []\n",
        "for a, b in zip(corpus_list, tag_list):\n",
        "    if len(a) != len(b):\n",
        "        remove_a.append(a)\n",
        "        remove_b.append(b)\n",
        "\n",
        "[corpus_list.remove(v) for v in remove_a]\n",
        "[tag_list.remove(v) for v in remove_b]\n",
        "\n",
        "print('Corpus size (filtered): {}'.format(len(corpus_list)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VPgJiaTxLAfI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word2vec embeddings"
      ]
    },
    {
      "metadata": {
        "id": "hQrsP3L8iFIk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wv_filename = 'corpus_model.wv'\n",
        "wv_path = wv_filename"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GYffRta8hoJt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train and save"
      ]
    },
    {
      "metadata": {
        "id": "DHM1--2efOxv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# train word2vec model\n",
        "model = Word2Vec(corpus_list, size=100, window=5, min_count=1, workers=8)\n",
        "\n",
        "# save corpus word embedding to drive\n",
        "model.wv.save(wv_path)\n",
        "\n",
        "# save some memory\n",
        "word_vectors = model.wv\n",
        "del model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IeGKLWnRhtd3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load pretrained"
      ]
    },
    {
      "metadata": {
        "id": "f8xBAmeNhjxX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "word_vectors = KeyedVectors.load(wv_path, mmap='r')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wQWBaepkXQLj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define BiLSTM model"
      ]
    },
    {
      "metadata": {
        "id": "PYwpJ03SfOx_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ]
    },
    {
      "metadata": {
        "id": "GxwVXV0dfOyB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "# def prepare_sequence(seq, to_ix):\n",
        "#     idxs = [to_ix[w] for w in seq]\n",
        "#     return torch.tensor(idxs, dtype=torch.long)\n",
        "def prepare_sequence(seq, word_vectors):\n",
        "    # return word2vec word embedding\n",
        "    idxs = [word_vectors.index2word.index(w) for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "  \n",
        "# Gets output of model from data\n",
        "def get_classes(data, model):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sentence, tags in training_data:\n",
        "            if len(sentence) != len(tags):\n",
        "                print(sentence)\n",
        "\n",
        "            true = np.array([tag_to_ix[t] for t in tags])\n",
        "            true = (true != 2).tolist()\n",
        "            y_true.extend(true)\n",
        "\n",
        "            pred = np.array(model(prepare_sequence(sentence, word_vectors))[1])\n",
        "#             pred = np.array(model(prepare_sequence(sentence, word_to_ix))[1])\n",
        "            pred = (pred != 2).tolist()\n",
        "            y_pred.extend(pred)\n",
        "\n",
        "    return (y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QBJ3f6vCfOyI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BiLSTM class\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "z1UulwHqfOyK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_vectors, tag_to_ix, hidden_dim):\n",
        "#     def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "#         self.embedding_dim = embedding_dim\n",
        "        self.embedding_dim = embedding_vectors.vector_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = len(embedding_vectors.vocab)\n",
        "#         self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "        self.word_embeds = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_vectors.vectors))\n",
        "#         self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the previous step, plus the score of transitioning from tag i to next_tag. We don't include the emission scores here because the max does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "orlulwXvfOyS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train BiLSTM model\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JkFzcivIj7FX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(word_vectors, tag_to_ix, HIDDEN_DIM, learning_rate, num_epochs, training_data):\n",
        "\n",
        "  def slack_print(msg):\n",
        "    print(msg)\n",
        "#     slack.chat.post_message(channel='nlp-disease-model',\n",
        "#                             text=msg,\n",
        "#                             username='LSTM-log')\n",
        "    \n",
        "  def slack_savefig(fig, fname, fmt):\n",
        "    fig.savefig(fname=fname, fmt=fmt)\n",
        "#     slack.files.upload(file_=fname,\n",
        "#                        channels='nlp-disease-model', \n",
        "#                        initial_comment=fname)\n",
        "    \n",
        "  slack_print('`START OF TRAINING`')\n",
        "  slack_print('training size: {}'.format(len(training_data)))\n",
        "  slack_print('learning rate: {}'.format(learning_rate))\n",
        "  slack_print('epochs: {}'.format(num_epochs))\n",
        "  \n",
        "  fname_params = 'train-size-{}.lr-{}.epochs-{}'.format(num_train, learning_rate, num_epochs)\n",
        "  \n",
        "  # Initialize model & optimizer\n",
        "  model = BiLSTM_CRF(word_vectors, tag_to_ix, HIDDEN_DIM)\n",
        "  # model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "  # Training...\n",
        "  # Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
        "  hist = np.zeros(num_epochs)\n",
        "  outputs = []\n",
        "  for epoch in range(num_epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "      start = time.time()\n",
        "      for sentence, tags in training_data:\n",
        "          # Step 1. Remember that Pytorch accumulates gradients. We need to clear them out before each instance\n",
        "          model.zero_grad()\n",
        "\n",
        "          # Step 2. Get our inputs ready for the network, that is, turn them into Tensors of word indices.\n",
        "          sentence_in = prepare_sequence(sentence, word_vectors)\n",
        "  #         sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "          targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
        "\n",
        "          # Step 3. Run our forward pass.\n",
        "          loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "          # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "          # calling optimizer.step()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      end = time.time()\n",
        "      log_msg = ' '.join([\"Epoch \", str(epoch), \" | MSE: \", str(loss.item()), ' | Time elapsed: ', str(end - start)])\n",
        "      slack_print(log_msg)\n",
        "      hist[epoch] = loss.item()\n",
        "      outputs.append(get_classes(training_data, model))\n",
        "\n",
        "      # Plot loss\n",
        "      loss_fig = plt.figure()\n",
        "      plt.plot(hist)\n",
        "      slack_savefig(loss_fig, fname='.loss.' + fname_params + '.png', fmt='png')\n",
        "\n",
        "      # Calculate F1 scores\n",
        "      f1_macro = []\n",
        "      f1_micro = []\n",
        "      for y_true, y_pred in outputs:\n",
        "        f1_macro.append(metrics.f1_score(y_true, y_pred, average='macro'))\n",
        "        f1_micro.append(metrics.f1_score(y_true, y_pred, average='micro'))\n",
        "\n",
        "      # Plot F1 scores over epochs\n",
        "      f1_fig = plt.figure()\n",
        "      model_metrics = pd.DataFrame([range(num_epochs), f1_macro, f1_micro], index=['epoch', 'F1 (macro)', 'F1 (micro)']).T\n",
        "      sns.lineplot(x='epoch', y='value', hue='variable', style='variable', dashes=False, data=model_metrics.melt(id_vars='epoch'))\n",
        "      sns.despine()\n",
        "      plt.ylim(0,1)\n",
        "      \n",
        "      slack_savefig(f1_fig, fname=drive_dir  + '.f1_scores.' + fname_params + '.png', fmt='png')\n",
        "\n",
        "      \n",
        "      # Save trained model\n",
        "      torch.save(model.state_dict(), 'model.{}.pickle'.format(fname_params))\n",
        "\n",
        "  with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[0][0], word_vectors)\n",
        "    tag_scores = model(inputs)\n",
        "    slack_print(tag_scores)\n",
        "  \n",
        "  slack_print('<!ckmah> `TRAINING DONE.`')\n",
        "  return hist, outputs, loss_fig, f1_fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Bv7tSqFkNJI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define parameters and train\n",
        "\n",
        "**NOTE:** do not run unless retraining model"
      ]
    },
    {
      "metadata": {
        "id": "LwL1LPAIpVEx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "EMBEDDING_DIM = 5\n",
        "HIDDEN_DIM = 4\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cJszmBJ0osau",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define training and test data\n",
        "training_data = [(sentence, tags) for sentence, tags in zip(corpus_list[:num_train], tag_list[:num_train])]\n",
        "test_data = [(sentence, tags) for sentence, tags in zip(corpus_list[num_train:], tag_list[num_train:])]\n",
        "\n",
        "\n",
        "results = train_model(word_vectors, tag_to_ix, HIDDEN_DIM, learning_rate, num_epochs, training_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vQlYNfK-nvz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "metadata": {
        "id": "qjClooqmgruq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "c4866d8f-3ea8-4f85-de91-0ed07be4fe0a"
      },
      "cell_type": "code",
      "source": [
        "model = BiLSTM_CRF(word_vectors, tag_to_ix, HIDDEN_DIM)\n",
        "model.load_state_dict(torch.load('model.train-size-100.lr-0.008.epochs-30.pickle'))\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM_CRF(\n",
              "  (word_embeds): Embedding(14406, 100)\n",
              "  (lstm): LSTM(100, 2, bidirectional=True)\n",
              "  (hidden2tag): Linear(in_features=4, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "e0wdiJVrOAUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "e5ac37c8-ad1a-4589-e52d-b543c7ab745c"
      },
      "cell_type": "code",
      "source": [
        "training_pred = get_classes(training_data, model)\n",
        "print('F1-macro:', metrics.f1_score(training_pred[0], training_pred[1], average='macro'))\n",
        "print('F1-micro:', metrics.f1_score(training_pred[0], training_pred[1], average='micro'))\n",
        "print('Recall: ', metrics.recall_score(training_pred[0], training_pred[1]))\n",
        "print('Precision: ', metrics.precision_score(training_pred[0], training_pred[1]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-macro: 0.5750981969422336\n",
            "F1-micro: 0.9380308109140802\n",
            "Recall:  0.10164729919550504\n",
            "Precision:  0.887402452619844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-4-8g2deJun_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "4f18d099-1372-4be8-a7ee-a1f62135a02b"
      },
      "cell_type": "code",
      "source": [
        "test_pred = get_classes(test_data, model)\n",
        "print('Test data:')\n",
        "print('F1-macro:', metrics.f1_score(test_pred[0], test_pred[1], average='macro'))\n",
        "print('F1-micro:', metrics.f1_score(test_pred[0], test_pred[1], average='micro'))\n",
        "print('Recall: ', metrics.recall_score(test_pred[0], test_pred[1]))\n",
        "print('Precision: ', metrics.precision_score(test_pred[0], test_pred[1]))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data:\n",
            "F1-macro: 0.5751365428442881\n",
            "F1-micro: 0.938056862983483\n",
            "Recall:  0.10164729919550504\n",
            "Precision:  0.8903803131991052\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}